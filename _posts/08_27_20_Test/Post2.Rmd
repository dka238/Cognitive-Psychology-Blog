---
title: "Post 2: Introduction Readings"
description: |
  Reflections on Introduction Readings.
author:
  - name: Diogo Koch Alves
    url: https://dka238.github.io/Cognitive-Psychology-Blog/
    affiliation: Baruch College
    affiliation_url: https://www.baruch.cuny.edu/
date: 08-27-2020
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Learn more about creating blogs with Distill at:
# https://rstudio.github.io/distill/blog.html

```

# Miller (1994)

I've been familiar with this paper and always default to using seven-point Likert scales in my research. It seems to hit the sweet spot with minimizing irrelevant noise but still allowing for precision. Now that I've read the paper more carefully, I have more context and concrete evidence for why seven seems to be the ideal number. I'm actually a big questionnaire design nerd and have taught best practices to grad students in the past. I generally focus on question wording/framing, question order, and flow and have borrowed heavily from papers like Nisbett & Wilson (1977) and Schwarz (1999). I'm glad to now include Miller (1994) more firmly in my arsenal.

One question that arose for me as I was reading this paper was: why do we see so many questionnaires that use 100-point sliding scales? I have posed this question before because it seems to me that such a strategy would capture a lot of meaninglses noise, and I was told that these types of questions are suitable for experts. For example, asking doctors about medically related questions would allow for greater precision of their answers since that is their expertise. Jury's still out for me if that's a good strategy since I haven't yet seen any substantive support for it. 


# Schacter, Eich, & Tulving (1978)

This was quite a read. Having little exposure to memory research, I was struck by the amount of information and am admittedly still processing it all. I did have a few thoughts about the paper that I wanted to share, however:

* The notion that "engrams" (i.e., memory traces) are stored through physiochemical processes had me raising my eyebrows. If Semon argued that engrams are physical, then it doesn't seem to far a logical jump to argue that thoughts themselves are physical. Clearly this is not (likely to be) the case. But it is an interesting notion, and does seem to engender the findings of activity in neuronal networks. 
* When the authors discuss temporal organization, I noticed that they mentioned "two components of the acoluthic phase". Namely, a period of short-lived activity and a longer phase of subconscious activity. Although no explicitly mentioned, could these be the antecedents to working memory, short-term memory, and long-term memory?
* They mention that a few of the reasons that Semon's work is largely unknown today include its aparadigmatic nature, his invention of terminology, and lack of original experimentation. I'm not entirely sure I'm convinced by these reasons. First, while I have found myself caught in the Catch-22 of observing a real-world phenomenon for which there is no theoretical framework to root in, I imagine that the creation of a theoretical "superstructure" is not impossible. Second, many scientists have invented terminology and in fact, many scientists have different terminology for the exact same concepts! Lastly, many of the founders of psychology did not have original experimentaiton. William James, for example, wrote almost exclusive in prose about consciousness and yet he is extrememly well known in the field.

# Newell (1973)

I have to be honest, I was unsure how to follow this paper. It felt a little bit like an op-ed on a series of papers which I did not have access to. I think the biggest take-away for me from this paper was Newell's comments on the Wingfield & Barnes (1972) paper on dichotic listening. The paper essentially shows that sequential grouping of stimuli (e.g., left ear gets stimuli L1, L2, L3 and then right ear gets stimuli R1, R2, R3) is more easily processed and recalled than simultaneous grouping (e.g., L1, R1, L2, R2, L3, R3). To me this raises the questions: why does this happen? What is the underlying mechanism? What are downstream consequences of this? Does it have to do with localization?   